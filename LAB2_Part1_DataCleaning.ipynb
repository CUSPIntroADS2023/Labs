{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Required Packages:**\n",
    "\n",
    "pandas, numpy, urllib (version stated in the next cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restart kernel after executing this cell \n",
    "pip install urllib3==1.26.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2. Part 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data can be messy with all sorts of issues like \n",
    "* missing or incorrect values (e.g. entries containing nothing, zero, nan or wrong quantity);\n",
    "* misformatted records and entries (e.g. entries as strings instead of numeric or date-time or wrong number of entries per row);\n",
    "* duplicate records and other.\n",
    "\n",
    "In short a rule of thumb is that nothing can be taken for granted and need to be verified with format, value and sanity checks. Some issue just won't let you upload the data.\n",
    "With others Python could let you through but as we'll see in the case in the second notebook, if ingored, those issues can be substaintial enough to completely derail the analysis.\n",
    "\n",
    "So data cleaning is an important first step in any data analyics project. Consider an example of some real-estate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data and visualize as text to see what's inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Data'):\n",
    "    !mkdir Data\n",
    "url = 'https://raw.githubusercontent.com/CUSP2020PUI/Data/master/RE_example11.csv'\n",
    "urllib.request.urlretrieve(url,'Data/RE_example11.csv')\n",
    "fname = 'Data/RE_example11.csv'\n",
    "f = open(fname, 'r')\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "f = requests.get(url)\n",
    "print(f.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may see:\n",
    "\n",
    "* some rows duplicated, \n",
    "* some having too many commas (perhaps due to using them to separate groups of digits in addition to separating fields),\n",
    "* missing values,\n",
    "* zero values\n",
    "* none/nan values\n",
    "* inconsistent date formats\n",
    "\n",
    "This is of course a toy example where all those issues were deliberately \"concetrated\" within a small sample for illustration purposes, but you can expect similar issues with real-world data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv('data/RE_example11.csv') #if we try to read the data it will through an error due to format inconsistency between rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales = pd.read_csv('Data/RE_example11.csv', on_bad_lines='skip') #so use on_bad_liness flag to instruct pandas to skip the misformatted lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales #the data is read succesfully, while losing some lines; but some issue are still there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales.describe() #use basic descriptive analysis to spot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we see LAND SQUARE FEET is not included, \n",
    "#meaining that it is perhaps treated as a string field, rather than date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales['LAND SQUARE FEET'] = pd.to_numeric(re_sales['LAND SQUARE FEET'], errors='coerce') #convert to numeric, turning invalid parsing to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(re_sales['SALE DATE'].min(), re_sales['SALE DATE'].max()) #also if we try getting a range for SALE DATE it does not work properly giving us text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales['SALE DATE'] = pd.to_datetime(re_sales['SALE DATE']) #convert to data-time; it unifies variety of formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now descriptive analysis works as intended\n",
    "(re_sales['SALE DATE'].min(), re_sales['SALE DATE'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales.describe() #however min values are 0 for the fiels that should not have zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#introduce basic sanity filtering, excluduing zero values\n",
    "sanityindex = (re_sales['SALE PRICE'] > 0) & (re_sales['YEAR BUILT'] > 0)\n",
    "re_sales = re_sales.loc[sanityindex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales = re_sales.loc[sanityindex]\n",
    "re_sales.describe() #still min value for the sale price remains unrealistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanityindex = (re_sales['SALE PRICE'] > 5000) #remove properties worth less than 5000 (in the following case we'll see how one can get some insights on this kind of filtering)\n",
    "re_sales = re_sales.loc[sanityindex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two remaining issues are nan values and duplicate rows address those below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales = re_sales.loc[sanityindex].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales = pd.DataFrame.drop_duplicates(re_sales) \n",
    "#new pandas versions have .drop_duplicates as a method of a dataframe enabling re_sales.drop_duplicated(inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales #finally we can see that the index now have gaps as we dropped quite a few records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_sales.reset_index(inplace = True, drop = True) #if we want it consistent we can reset index\n",
    "re_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only 16 out of 41 records survived the data cleaning. It is not always that bad, but as we'll see from the following real world case if can sometimes be even worse..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please note that this is just a toy dataset, you don't need to follow the exactly the same data cleaning steps \n",
    "# when working on a real citibike dataset\n",
    "url = 'https://raw.githubusercontent.com/CUSP2020PUI/Data/master/citibike.csv'\n",
    "citibike = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task 1\n",
    "Filter out trips with unreasonal trip duration. Trip duration has to be a positive number, and shorter than 3 hours.\n",
    "\n",
    "Hint: \n",
    "    1. convert starttime, endtime to timestamp at first\n",
    "    2. use .astype('timedelta64[m]') to get trip duration in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task 2\n",
    "Remove trip records which include unrealistic costumer age.\n",
    "\n",
    "Hint:\n",
    "    1. Customer age has to be less than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task 3\n",
    "drop duplicated records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task 4\n",
    "Find records where start location or end location is outside of New York City, then delete them.\n",
    "\n",
    "NYC latitude, longitude range is available at https://www1.nyc.gov/assets/planning/download/pdf/data-maps/open-data/nybb_metadata.pdf?ver=18c as\n",
    "    \n",
    "    West -74.257159 East -73.699215\n",
    "    North 40.915568 South 40.495992"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
